{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots\n",
    "using Statistics\n",
    "using Random\n",
    "using ForwardDiff\n",
    "using Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.0008073276783183449\n",
      "var: 0.9990963717486129\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "alpha_1 = 0.9\n",
    "alpha_2 = -0.2\n",
    "beta = 0.8\n",
    "sigma_1 = 1.0\n",
    "sigma_2 = 1.0\n",
    "sigma_12 = 0.5\n",
    "\n",
    "# Initial conditions (shouldn't matter too much)\n",
    "x_0 = 0.0\n",
    "y_0 = 0.0\n",
    "\n",
    "# DGP conditions\n",
    "T = 1000000\n",
    "Random.seed!(1234)\n",
    "\n",
    "# Generate data\n",
    "X = x_0 .* ones(T) .* 0\n",
    "Y = y_0 .* ones(T) .* 0\n",
    "W = randn(T,2)\n",
    "\n",
    "# Spot check\n",
    "println(\"mean: \", mean(W))\n",
    "println(\"var: \", var(W))\n",
    "\n",
    "# Build in a detailed but bad way\n",
    "for t in 1:T-1\n",
    "    Y[t+1] = alpha_1*Y[t] + alpha_2*X[t] + sigma_1*W[t+1,1] + sigma_12*W[t+1,2]\n",
    "    X[t+1] = beta*X[t] + sigma_2*W[t+1,2]\n",
    "end\n",
    "\n",
    "# With matrices\n",
    "Z = [y_0 .* ones(T) x_0 .* ones(T)]\n",
    "\n",
    "# Parameters\n",
    "A = [alpha_1 alpha_2 ; 0 beta]\n",
    "B = [sigma_1 sigma_12 ; 0 sigma_2]\n",
    "\n",
    "# Build in a simple, good way\n",
    "for t in 1:T-1\n",
    "    Z[t+1,:] = A*Z[t,:] + B*W[t+1,:]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for data construction\n",
    "# pX = plot(X)\n",
    "# plot!(Z[:,2])\n",
    "# display(pX)\n",
    "\n",
    "# pY = plot(Y)\n",
    "# plot!(Z[:,1])\n",
    "# display(pY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simp_mat: 999095.9528099515\n",
      "Iter     Function value   Gradient norm \n",
      "     0     3.758276e+06     6.128020e+06\n",
      " * time: 0.00030493736267089844\n",
      "     1     9.990951e+05     9.214318e-10\n",
      " * time: 3.356060028076172\n",
      "alpha_1_hat: 0.900512988889434\n",
      "FI simp mat hess: [6.805032374304787]\n",
      "\n",
      "full_mat: 999095.9528099515\n",
      "Iter     Function value   Gradient norm \n",
      "     0     5.032770e+06     6.005163e+06\n",
      " * time: 0.0002880096435546875\n",
      "     1     1.181045e+06     1.041502e+06\n",
      " * time: 2.9404799938201904\n",
      "     2     1.017153e+06     2.396884e+05\n",
      " * time: 5.616627931594849\n",
      "     3     9.990949e+05     6.046434e-10\n",
      " * time: 8.112473964691162\n",
      "theta_hat: [0.9005077387073848, -0.19997570010958343, 0.7996996321071902]\n",
      "FI for alpha_1: 6.789907946129881\n"
     ]
    }
   ],
   "source": [
    "# Calculate log likelihood\n",
    "function log_lik_mat(A,B,Z)\n",
    "    return -1/2 * sum([(B \\ (Z[t+1,:] - A*Z[t,:]))'*(B \\ (Z[t+1,:] - A*Z[t,:])) for t in 1:size(Z,1)-1])\n",
    "end\n",
    "\n",
    "# negative of log-lik, as function of alpha_1 only\n",
    "simp_mat(a) = -log_lik_mat([a alpha_2; 0 beta], [sigma_1 sigma_12; 0 sigma_2], Z)\n",
    "println(\"simp_mat: \", simp_mat(alpha_1))\n",
    "\n",
    "# Estimate alpha_1_hat by MLE\n",
    "alpha_1_hat = optimize(simp_mat, [0.0], BFGS(), Optim.Options(show_trace = true); autodiff = :forward).minimizer[1]\n",
    "println(\"alpha_1_hat: \", alpha_1_hat)\n",
    "\n",
    "# Calculate gradient and Hessian of loglik\n",
    "grad_simp_mat(a) = ForwardDiff.gradient(simp_mat, [a])\n",
    "hess_simp_mat(a) = ForwardDiff.hessian(simp_mat, [a])\n",
    "# println(\"grad_mat: \", grad_simp_mat(alpha_1))\n",
    "# println(\"FI simp mat grad: \", (grad_simp_mat(alpha_1)'*grad_simp_mat(alpha_1)))\n",
    "\n",
    "# Fisher Info (normalize by T)\n",
    "println(\"FI simp mat hess: \", (1/T * hess_simp_mat(alpha_1_hat)))\n",
    "\n",
    "println()\n",
    "\n",
    "# negative of loglik, as function of alpha_1, alpha_2, beta\n",
    "full_mat(theta) = -log_lik_mat([theta[1] theta[2]; 0 theta[3]], [sigma_1 sigma_12; 0 sigma_2], Z)\n",
    "theta_0 = [alpha_1 alpha_2 beta]\n",
    "println(\"full_mat: \", full_mat(theta_0))\n",
    "\n",
    "# Estimate theta by MLE\n",
    "theta_hat = optimize(full_mat, [0.0, 0.0, 0.0], BFGS(), Optim.Options(show_trace = true); autodiff = :forward).minimizer\n",
    "println(\"theta_hat: \", theta_hat)\n",
    "\n",
    "# Get grad and Hess of loglik\n",
    "grad_full_mat(theta) = ForwardDiff.gradient(full_mat, theta)\n",
    "hess_full_mat(theta) = ForwardDiff.hessian(full_mat, theta)\n",
    "\n",
    "# \"Fisher Info\" (normalize by T)\n",
    "println(\"FI for alpha_1: \", 1/((1/T * hess_full_mat(theta_hat) \\ I)[1,1]))\n",
    "\n",
    "# We ought to have less info when we have nuisances (it seems this holds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discarded Code that I wanted to hold onto (just in case)\n",
    "\n",
    "# function log_lik(theta, sigma, X, Y)\n",
    "#     a1 = theta[1]\n",
    "#     a2 = theta[2]\n",
    "#     b = theta[3]\n",
    "    \n",
    "#     s1 = sigma[1]\n",
    "#     s2 = sigma[2]\n",
    "#     s12 = sigma[3]\n",
    "    \n",
    "#     T = length(Y)\n",
    "# #     println(\"size: \", size((1/s1 .* (Y[2:T] .- a1 .* Y[1:T-1] .- a2 .* X[1:T-1] .- s12/s2 .* (X[2:T] .- b .* X[1:T-1]))).^ 2))\n",
    "    \n",
    "#     return -1/2*sum((1/s1 .* (Y[2:T] - a1 .* Y[1:T-1] - a2 .* X[1:T-1] - s12/s2 .* (X[2:T] - b .* X[1:T-1]))).^ 2)\n",
    "# end\n",
    "\n",
    "# println(\"full: \", log_lik([alpha_1 alpha_2 beta], [sigma_1 sigma_2 sigma_12], X, Y))\n",
    "\n",
    "# simp_ll(a) = log_lik([a alpha_2 beta], [sigma_1 sigma_2 sigma_12], X, Y)\n",
    "# println(\"simp_ll: \", simp_ll(alpha_1))\n",
    "# grad_simp_ll(a) = ForwardDiff.gradient(simp_ll, [a])\n",
    "# hess_simp_ll(a) = ForwardDiff.hessian(simp_ll, [a])\n",
    "\n",
    "# println(\"grad: \", grad_simp_ll(alpha_1))\n",
    "# println(\"FI simp grad: \", (grad_simp_ll(alpha_1)'*grad_simp_ll(alpha_1)) \\ I)\n",
    "# println(\"FI simp hess: \", (-hess_simp_ll(alpha_1)) \\ I)\n",
    "# println()\n",
    "\n",
    "# full_ll(theta) = log_lik(theta, [sigma_1 sigma_2 sigma_12], X, Y)\n",
    "# grad_full_ll(theta) = ForwardDiff.gradient(full_ll, theta)\n",
    "# hess_full_ll(theta) = ForwardDiff.hessian(full_ll, theta)\n",
    "\n",
    "# theta_0 = [alpha_1 alpha_2 beta]\n",
    "\n",
    "# println(\"full_ll: \", full_ll(theta_0))\n",
    "# println(\"grad_full test: \", grad_full_ll(theta_0))\n",
    "# println(\"grad_full_ll: \", (grad_full_ll(theta_0)'*grad_full_ll(theta_0)))\n",
    "# println(\"FI full grad: \", (grad_full_ll(theta_0)'*grad_full_ll(theta_0)))\n",
    "# println(\"FI full hess: \", (-hess_full_ll(theta_0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
